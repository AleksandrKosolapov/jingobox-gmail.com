{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OneTwoTrip предлагает решить две задачи по предсказанию поведения клиентов на сайте. Датасет общий на обе задачи и содержит анонимизированную информацию о пользователях и их действиях на сайте.\n",
    "Медали и рейтинг в общий зачет присуждаются отдельно за каждую задачу. Финальный лидерборд строится после публикации приватного рейтинга по обеим задачам. Участники, занявшие 1-10 места в задачах, получат от 10 до 1 балла соответственно за каждую задачу, после баллы суммируются. При равенстве баллов у участников приоритет отдается решению второй задачи.\n",
    "\n",
    "Задача 1:\n",
    "Если у пассажира изменились планы или по каким-то причинам он не сможет воспользоваться перелетом, то он может вернуть авиабилет. Для этого ему надо создать заявку на возврат. Для каждого заказа (orderid) предскажите с какой вероятностью пользователь подаст заявку на возврат билета. В качестве сабмита принимается .csv файл с двумя колонками - id заказа (orderid) и вероятностью возврата. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#загрузка необходимых библиотек\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import cross_val_score,GroupKFold\n",
    "from sklearn import preprocessing\n",
    "from catboost import Pool, CatBoostClassifier\n",
    "import lightgbm as lgb\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn')\n",
    "%matplotlib inline\n",
    "from IPython.core.display import display, HTML\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#загружаем train и test\n",
    "train = pd.read_csv('onetwotrip_challenge_train.csv')\n",
    "train = train.sort_values(['userid','field4']).reset_index(drop=True) # сортируем трейн по юзерам и номерам их покупок билетов\n",
    "test = pd.read_csv('onetwotrip_challenge_test.csv')\n",
    "\n",
    "#кодируем айдишники юзеров числами\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(pd.concat([train['userid'],test['userid']]))\n",
    "train['userid'] = le.transform(train['userid'])\n",
    "test['userid'] = le.transform(test['userid'])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Field0: количество дней с предыдущей покупки (одного или нескольких билетов)\n",
    "Field1: общая стоимость покупки\n",
    "Field2: месяц оформления билета\n",
    "Field3: месяц вылета\n",
    "Field4: номер ордера для каждого userid,\n",
    "Field5: 0/1: оналйн-регистрация или покупка билета по промокоду\n",
    "Field6: ? max:323\n",
    "Field7: ? 1/2\n",
    "Field8: ? 0/1\n",
    "Field9: количество билетов для детей возрастом до 1 года\n",
    "Field10: ? 0/1\n",
    "Field11: час вылета/оформления билета\n",
    "Field12: ? max: 273\n",
    "Field13: ? max: 746\n",
    "Field14: ? max: 401\n",
    "Field15: количество всех билетов в покупке, равное сумме трёх field*: 24, 28, 9.\n",
    "Field16: количество дней до вылета\n",
    "Field17: max: 156\n",
    "Field18: день оформления билета\n",
    "Field19: ? 2-68939 раз,1-64295,3-50883,4-7419,5-3709,6-619,0-151,7-39,8-2\n",
    "Field20: день вылета (день недели)\n",
    "Field21: год покупки 2-первый, 1 - второй\n",
    "Field22: ? max:867\n",
    "Field23: час покупки\n",
    "Field24: взрослых билетов\n",
    "Field25: ? max: 170\n",
    "Field26: ? 28 значений\n",
    "Field27: ? [1,2,3,4,5,6]\n",
    "Field28: детей от 2 до 6\n",
    "Field29: ? [1,2,3,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fff = [x for x in train.columns if x not in ['goal21', 'goal22', 'goal23', 'goal24', 'goal25', 'goal1','orderid','userid']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# В датасете данные за 2 года, отдельная фича означает 0 - первый год, 1 - второй. \n",
    "# Добавим фичи с месяцами покупки и вылета ко второму году\n",
    "\n",
    "for df in [train,test]:\n",
    "    df.loc[df['field21']==1,'field2'] = df['field2'] + 12 # Добавим фичу с месяцем покупки ко второму году\n",
    "    df.loc[df['field21']==1,'field3'] = df['field3'] + 12 # Добавим фичу с месяцем вылета ко второму году\n",
    "    df['ticket_price'] = df['field1'] / df['field15'] # средняя цена одного билета в покупке\n",
    "    \n",
    "    df['userid'] = le.transform(df['userid']) # не конкатенируем, т.к. юзеры трейна и теста не пересекаются\n",
    "    df['COUNT_id'] = df['userid'].map(df['userid'].value_counts()) # заменим userid на каунты\n",
    "\n",
    "\n",
    "#Посчитаем некоторые эффективные статистики: \n",
    "col = 'userid'\n",
    "for df in [train,test]:\n",
    "    for f in ['ticket_price']:\n",
    "        df['{}_{}_STD'.format(col,f)] = df[col].map(pd.concat([train[[col,f]],test[[col,f]]]).groupby(col)[f].std()) #del train['ticket_price'],test['ticket_price']\n",
    "   \n",
    "    for f in ['field16','field0','field23']: #22,11,13,1,12,price,3,6,20,2,18\n",
    "        df['{}_max'.format(f)] = df[col].map(df.groupby(col)[f].max())\n",
    "        df['{}_min'.format(f)] = df[col].map(df.groupby(col)[f].min())\n",
    "        df['{}_razmah'.format(f)] = df['{}_max'.format(f)]- df['{}_min'.format(f)]\n",
    "        del df['{}_min'.format(f)],df['{}_max'.format(f)]\n",
    "        \n",
    "for df in [train,test]:\n",
    "    for f in ['field26']:\n",
    "        df['{}_{}_STD'.format(col,f)] = df[col].map(pd.concat([train[[col,f]],test[[col,f]]]).groupby(col)[f].std())\n",
    "        df['{}_{}_MEAN'.format(col,f)] = df[col].map(pd.concat([train[[col,f]],test[[col,f]]]).groupby(col)[f].mean())\n",
    "        df['{}_{}_MAX'.format(col,f)] = df[col].map(pd.concat([train[[col,f]],test[[col,f]]]).groupby(col)[f].max())\n",
    "\n",
    "for df in [train,test]:\n",
    "    for f in ['field0']:\n",
    "        df['{}_{}_STD'.format(col,f)] = df[col].map(pd.concat([train[[col,f]],test[[col,f]]]).groupby(col)[f].std())\n",
    "for df in [train,test]:\n",
    "    for f in ['field13']:\n",
    "        df['{}_{}_STD'.format(col,f)] = df[col].map(pd.concat([train[[col,f]],test[[col,f]]]).groupby(col)[f].std())\n",
    "for df in [train,test]:\n",
    "    for f in ['field1']:\n",
    "        df['{}_{}_MEAN'.format(col,f)] = df[col].map(pd.concat([train[[col,f]],test[[col,f]]]).groupby(col)[f].mean())\n",
    "for df in [train,test]:\n",
    "    for f in ['field16']:\n",
    "        df['{}_{}_MEAN'.format(col,f)] = df[col].map(pd.concat([train[[col,f]],test[[col,f]]]).groupby(col)[f].mean())\n",
    "        df['{}_{}_MIN'.format(col,f)] = df[col].map(pd.concat([train[[col,f]],test[[col,f]]]).groupby(col)[f].min())\n",
    "for df in [train,test]:\n",
    "    for f in ['field12']:\n",
    "        df['{}_{}_STD'.format(col,f)] = df[col].map(pd.concat([train[[col,f]],test[[col,f]]]).groupby(col)[f].std())\n",
    "for df in [train,test]:        \n",
    "    for f in ['field23','ticket_price']:\n",
    "        df['first_ticket_{}'.format(f)] = df['userid'].map(df[df['field4']==1][['userid',f]].set_index('userid')[f].to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Брутфорс поиска эффективных фич выявил что каунты категориальной фичи: 'field13' + 'field22' дают ощутимый прирост качеству модели.\n",
    "\n",
    "for f1 in ['field13']:\n",
    "    for f2 in ['field22']:\n",
    "        if f1 != f2:\n",
    "            train['COUNT_{}_{}'.format(f1,f2)] = train[f1].astype(str) + \"_\" + train[f2].astype(str)\n",
    "            test['COUNT_{}_{}'.format(f1,f2)] = test[f1].astype(str) + \"_\" + test[f2].astype(str)\n",
    "            train['COUNT_{}_{}'.format(f1,f2)] = train['COUNT_{}_{}'.format(f1,f2)].map(pd.concat([train['COUNT_{}_{}'.format(f1,f2)],test['COUNT_{}_{}'.format(f1,f2)]]).value_counts())\n",
    "            test['COUNT_{}_{}'.format(f1,f2)] = test['COUNT_{}_{}'.format(f1,f2)].map(pd.concat([train['COUNT_{}_{}'.format(f1,f2)],test['COUNT_{}_{}'.format(f1,f2)]]).value_counts())\n",
    "            \n",
    "for f in ['field12','field16']:  \n",
    "    train['COUNT_{}'.format(f)] = train[f].map(pd.concat([train[f],test[f]]).value_counts())\n",
    "    test['COUNT_{}'.format(f)] = test[f].map(pd.concat([train[f],test[f]]).value_counts())        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# уменьшим размер нашего датасета\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Добавим отдельными признаками некоторые данные о первых покупках билетов и о последнем билете\n",
    "\n",
    "for f in ['field0','field16']:\n",
    "    for n in range(2,4):\n",
    "        train['{}_ticket_{}'.format(n,f)] = train['userid'].map(train[train['field4']==n][['userid',f]].set_index('userid')[f].to_dict())\n",
    "        test['{}_ticket_{}'.format(n,f)] = test['userid'].map(test[test['field4']==n][['userid',f]].set_index('userid')[f].to_dict())\n",
    "        train = reduce_mem_usage(train)\n",
    "        test = reduce_mem_usage(test)\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# переведём некоторые дискретные признаки в набор бинарных признаков\n",
    "train = pd.get_dummies(data=train,columns=['field27'])\n",
    "test = pd.get_dummies(data=test,columns=['field27'])\n",
    "\n",
    "#укажем NaN для тех статистик покупок, которых не более 1 у юзера\n",
    "y=2\n",
    "for col in train.columns:\n",
    "    if ('MIN' in col) or ('MAX' in col) or ('STD' in col) or ('MEAN' in col):\n",
    "        train.loc[train['COUNT_id']<y,col] = np.nan\n",
    "        test.loc[test['COUNT_id']<y,col] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target-encoding хорошо сработал для фичи field18\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.base import clone\n",
    "from sklearn.model_selection import check_cv\n",
    "from category_encoders import CatBoostEncoder\n",
    "\n",
    "y_train = train['goal1'].copy()\n",
    "X_train = train.drop(columns = ['goal21', 'goal22', 'goal23', 'goal24', 'goal25', 'goal1','orderid','userid'], axis=1)\n",
    "X_test = test.copy()\n",
    "\n",
    "class TargetEncoderCV(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, cv, **cbe_params):\n",
    "        self.cv = cv\n",
    "        self.cbe_params = cbe_params\n",
    "    @property\n",
    "    def _n_splits(self):\n",
    "        return check_cv(self.cv).n_splits\n",
    "    def fit_transform(self, X: pd.DataFrame, y) -> pd.DataFrame:\n",
    "        self.cbe_ = []\n",
    "        cv = check_cv(self.cv)\n",
    "        cbe = CatBoostEncoder(\n",
    "            cols=X.columns.tolist(),\n",
    "            return_df=False,\n",
    "            **self.cbe_params)\n",
    "        X_transformed = np.zeros_like(X, dtype=np.float64)\n",
    "        for train_idx, valid_idx in cv.split(X, y):\n",
    "            self.cbe_.append(\n",
    "                clone(cbe).fit(X.loc[train_idx], y[train_idx]))\n",
    "            X_transformed[valid_idx] = self.cbe_[-1].transform(X.loc[valid_idx])\n",
    "        return pd.DataFrame(X_transformed, columns=X.columns)\n",
    "    def transform(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        X_transformed = np.zeros_like(X, dtype=np.float64)\n",
    "        for cbe in self.cbe_:\n",
    "            X_transformed += cbe.transform(X) / self._n_splits\n",
    "        return pd.DataFrame(X_transformed, columns=X.columns)\n",
    "\n",
    "for trans_feature in ['field18']:\n",
    "    y_train = train['goal1'].copy()\n",
    "    X_train = train.drop(columns = ['goal21', 'goal22', 'goal23', 'goal24', 'goal25', 'goal1','orderid','userid'], axis=1)\n",
    "    X_test = test.copy()\n",
    "    X_train[trans_feature] = X_train[trans_feature].astype(str)\n",
    "    X_test[trans_feature] = X_test[trans_feature].astype(str)\n",
    "    te_cv = TargetEncoderCV(KFold(n_splits=3))\n",
    "    X_train = te_cv.fit_transform(X_train[[trans_feature]], y_train)\n",
    "    X_test = te_cv.transform(X_test[[trans_feature]])\n",
    "    train[trans_feature] = X_train[trans_feature]\n",
    "    test[trans_feature] = X_test[trans_feature]\n",
    "del X_train,X_test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "features = [c for c in train.columns if c not in ['field1','goal21', 'goal22', 'goal23', 'goal24', 'goal25', 'goal1','orderid','userid']]\n",
    "\n",
    "N=5\n",
    "rlist = [59,73,42,65,78]\n",
    "\n",
    "for r in rlist:\n",
    "    x_pred = np.zeros(train[features].shape[0])\n",
    "    y_pred = np.zeros(test[features].shape[0])\n",
    "    \n",
    "    #равномерно распределим записи в датасете, чтобы дисбаланс в каждом фолде был одинаковый\n",
    "    train = train.sort_values('goal1').reset_index(drop=True)\n",
    "    vc = train['goal1'].value_counts()\n",
    "    vc = dict(sorted(vc.items()))\n",
    "    df = pd.DataFrame()\n",
    "    train['indexcol'],i = 0,1\n",
    "    for k,v in vc.items():\n",
    "        step = train.shape[0]/v\n",
    "        indent = train.shape[0]/(v+1)\n",
    "        df2 = train[train['goal1'] == k].sample(v,random_state = r).reset_index(drop=True)\n",
    "        for j in range(0, v):\n",
    "            df2.at[j, 'indexcol'] = indent + j*step + 0.000001*i\n",
    "        df = pd.concat([df2,df])\n",
    "        i+=1\n",
    "    train = df.sort_values('indexcol', ascending=False).reset_index(drop=True)\n",
    "    del train['indexcol'], df, df2, vc\n",
    "    \n",
    "    #фолды делим kfold'ом\n",
    "    folds = GroupKFold(n_splits=N)\n",
    "    target = train['goal1'].copy()\n",
    "\n",
    "    lgb_param = {\n",
    "        'max_bin':24,\n",
    "        'min_data_in_leaf': 8,\n",
    "        'num_leaves': 128,\n",
    "        'learning_rate': 0.01,\n",
    "        'min_child_weight': 0.3,\n",
    "        'bagging_fraction': 0.4, \n",
    "        'feature_fraction': 0.595,\n",
    "        'max_depth': 7, \n",
    "        'objective': 'binary',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'verbose': 1,\n",
    "        'metric':'auc'}\n",
    "\n",
    "    importance = pd.DataFrame(np.zeros((train[features].shape[1], N)), columns=['Fold_{}'.format(i) for i in range(1, N + 1)], index=train[features].columns)\n",
    "    scores = []\n",
    "    oof = np.zeros(train[features].shape[0])\n",
    "\n",
    "    for fold, (trn_idx, val_idx) in enumerate(folds.split(train[features], target), 1):\n",
    "        print('Fold {}'.format(fold))\n",
    "        trn_data = lgb.Dataset(train[features].iloc[trn_idx, :].values, label=target.iloc[trn_idx].values)\n",
    "        val_data = lgb.Dataset(train[features].iloc[val_idx, :].values, label=target.iloc[val_idx].values)   \n",
    "        clf = lgb.train(lgb_param, trn_data, 500, valid_sets=[trn_data, val_data], verbose_eval=500, early_stopping_rounds=101)\n",
    "        predictions = clf.predict(train[features].iloc[val_idx, :].values) \n",
    "        importance.iloc[:, fold - 1] = clf.feature_importance()\n",
    "        oof[val_idx] = predictions\n",
    "        score = roc_auc_score(target.iloc[val_idx].values, predictions)\n",
    "        scores.append(score)\n",
    "        y_pred += clf.predict(test[features]) / (N*len(rlist))\n",
    "        x_pred += clf.predict(train[features]) / (N*len(rlist))\n",
    "        del trn_data, val_data, predictions\n",
    "        gc.collect()\n",
    "\n",
    "    print(r, '######### Average ROC AUC Score {} [STD:{}]'.format(np.mean(scores), np.std(scores)))\n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance['Mean_Importance'] = importance.sum(axis=1) / N\n",
    "importance = importance.sort_values(by='Mean_Importance', ascending=False)[:50]\n",
    "plt.figure(figsize=(15, 15))\n",
    "sns.barplot(x='Mean_Importance', y=importance.index, data=importance)\n",
    "plt.xlabel('')\n",
    "plt.tick_params(axis='x', labelsize=15)\n",
    "plt.tick_params(axis='y', labelsize=15)\n",
    "plt.title('Mean Feature Importance Between Folds', size=15)\n",
    "plt.savefig('{}.png'.format(sum(scores)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# формируем файл с предсказаниями\n",
    "pd.DataFrame(y_pred, columns=['proba'], index=test['orderid']).to_csv('submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "features = [c for c in train.columns if c not in ['field1','goal21', 'goal22', 'goal23', 'goal24', 'goal25', 'goal1','orderid','userid']]\n",
    "\n",
    "N=5\n",
    "rlist = [59,73,42]\n",
    "\n",
    "rlist = range(5)\n",
    "\n",
    "y_test_pred = 0\n",
    "for r in rlist:\n",
    "\n",
    "\n",
    "    train = pd.read_pickle('tr_1')\n",
    "    test = pd.read_pickle('te_1')\n",
    "    features = [c for c in train.columns if c not in ['field1','goal21', 'goal22', 'goal23', 'goal24', 'goal25', 'goal1','orderid','userid']]\n",
    "    \n",
    "    train = train.sort_values('goal1').reset_index(drop=True)\n",
    "    vc = train['goal1'].value_counts()\n",
    "    vc = dict(sorted(vc.items()))\n",
    "    df = pd.DataFrame()\n",
    "    train['indexcol'],i = 0,1\n",
    "    for k,v in vc.items():\n",
    "        step = train.shape[0]/v\n",
    "        indent = train.shape[0]/(v+1)\n",
    "        df2 = train[train['goal1'] == k].sample(v,random_state = r).reset_index(drop=True)\n",
    "        for j in range(0, v):\n",
    "            df2.at[j, 'indexcol'] = indent + j*step + 0.000001*i\n",
    "        df = pd.concat([df2,df])\n",
    "        i+=1\n",
    "    train = df.sort_values('indexcol', ascending=False).reset_index(drop=True)\n",
    "    del train['indexcol'], df, df2, vc\n",
    "\n",
    "    folds = KFold(n_splits=N)\n",
    "    target = train['goal1'].copy()\n",
    "\n",
    "    y_valid_pred = 0 * target\n",
    "  \n",
    "    for idx, (train_index, valid_index) in enumerate(kf.split(train)):\n",
    "\n",
    "        model = CatBoostClassifier(loss_function=\"Logloss\",\n",
    "               eval_metric=\"AUC\",\n",
    "               task_type=\"GPU\",\n",
    "               learning_rate=0.01,\n",
    "               iterations=10000,\n",
    "               od_type=\"Iter\",\n",
    "               depth=7,\n",
    "               early_stopping_rounds=300)\n",
    "        y_train, y_valid = target.iloc[train_index], target.iloc[valid_index]\n",
    "        X_train, X_valid = train[features].iloc[train_index,:], train[features].iloc[valid_index,:]\n",
    "        _train = Pool(X_train, label=y_train)\n",
    "        _valid = Pool(X_valid, label=y_valid)\n",
    "        print( \"\\nFold \", idx)\n",
    "        fit_model = model.fit(_train,\n",
    "                              eval_set=_valid,\n",
    "                              use_best_model=True,\n",
    "                              verbose=1000,\n",
    "                              plot=True\n",
    "                             )\n",
    "        pred = fit_model.predict_proba(X_valid)[:,1]\n",
    "        print( \"  auc = \", roc_auc_score(y_valid, pred) )\n",
    "        y_valid_pred.iloc[valid_index] = pred\n",
    "        y_test_pred += fit_model.predict_proba(test[features])[:,1]\n",
    "        gc.collect()\n",
    "        \n",
    "y_test_pred /= (N*len(rlist))\n",
    "        \n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(y_test_pred, columns=['proba'], index=test['orderid']).to_csv('catboost.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
